<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Funnel</title>
    <link>https://ohsu-comp-bio.github.io/funnel/</link>
    <description>Recent content on Funnel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://ohsu-comp-bio.github.io/funnel/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Batch</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/aws-batch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/aws-batch/</guid>
      <description>AWS Batch This guide covers deploying a Funnel server that leverages DynamoDB for storage and AWS Batch for task execution.
Setup Get started by creating a compute environment, job queue and job definition using either the Funnel CLI or the AWS Batch web console. To manage the permissions of instanced AWS Batch jobs create a new IAM role. For the Funnel configuration outlined in this document, this role will need to provide read and write access to both S3 and DynamoDB.</description>
    </item>
    
    <item>
      <title>Basic Auth</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/security/basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/security/basic/</guid>
      <description>Basic Auth By default, a Funnel server allows open access to its API endpoints, but it can be configured to require basic password authentication. To enable this, include a password in your config file:
Server: User: funnel Password: abc123  Make sure to properly protect the configuration file so that it&amp;rsquo;s not readable by everyone:
$ chmod 600 funnel.config.yml  To use the password, set the FUNNEL_SERVER_USER and FUNNEL_SERVER_PASSWORD environment variables:</description>
    </item>
    
    <item>
      <title>Compute</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/</guid>
      <description> Compute </description>
    </item>
    
    <item>
      <title>Databases</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/</guid>
      <description> Databases </description>
    </item>
    
    <item>
      <title>Datastore</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/datastore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/datastore/</guid>
      <description>Google Cloud Datastore Funnel supports storing tasks (but not scheduler data) in Google Cloud Datastore.
This implementation currently doesn&amp;rsquo;t work with Appengine, since Appengine places special requirements on the context of requests and requires a separate library.
Two entity types are used, &amp;ldquo;Task&amp;rdquo; and &amp;ldquo;TaskPart&amp;rdquo; (for larger pieces of task content, such as stdout/err logs).
Funnel will, by default, try to will try to automatically load credentials from the environment.</description>
    </item>
    
    <item>
      <title>Deploying a cluster</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/deployment/</guid>
      <description>Deploying a cluster This guide describes the basics of starting a cluster of Funnel nodes. This guide is a work in progress.
A node is a service which runs on each machine in a cluster. The node connects to the Funnel server and reports available resources. The Funnel scheduler process assigns tasks to nodes. When a task is assigned, a node will start a worker process. There is one worker process per task.</description>
    </item>
    
    <item>
      <title>Development</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/development/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/development/</guid>
      <description> Development </description>
    </item>
    
    <item>
      <title>Download</title>
      <link>https://ohsu-comp-bio.github.io/funnel/download/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/download/</guid>
      <description>Download  linux [funnel-linux-amd64-0.6.0.tar.gz] mac [funnel-darwin-amd64-0.6.0.tar.gz] Windows is not supported (yet), sorry!  Funnel is a single binary.
Funnel requires Docker.
Funnel is beta quality. APIs might break, bugs exist, data might be lost.
Homebrew brew tap ohsu-comp-bio/formula brew install funnel  Build the lastest development version optional In order to build the latest code, run:
$ go get github.com/ohsu-comp-bio/funnel  Funnel requires Go 1.8+. Check out the development docs for more detail.</description>
    </item>
    
    <item>
      <title>DynamoDB</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/dynamodb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/dynamodb/</guid>
      <description>DynamoDB Funnel supports storing task data in DynamoDB. Storing scheduler data is not supported currently, so using the node scheduler with DynamoDB won&amp;rsquo;t work. Using AWS Batch for compute scheduling may be a better option. Funnel will, by default, try to will try to automatically load credentials from the environment. Alternatively, you may explicitly set the credentials in the config.
Available Config:
Database: dynamodb DynamoDB: # Basename to use for dynamodb tables TableBasename: &amp;quot;funnel&amp;quot; # AWS region Region: &amp;quot;us-west-2&amp;quot; # AWS Access key ID Key: &amp;quot;&amp;quot; # AWS Secret Access Key Secret: &amp;quot;&amp;quot;  Known issues Dynamo does not store scheduler data.</description>
    </item>
    
    <item>
      <title>Elasticsearch</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/elasticsearch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/elasticsearch/</guid>
      <description> Elasticsearch Funnel supports storing tasks and scheduler data in Elasticsearch.
Config:
Database: elastic Elastic: # Prefix to use for indexes IndexPrefix: &amp;quot;funnel&amp;quot; URL: http://localhost:9200  </description>
    </item>
    
    <item>
      <title>Embedded</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/boltdb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/boltdb/</guid>
      <description> Embedded By default, Funnel uses an embedded database named BoltDB to store task and scheduler data. This is great for development and a simple server without external dependencies, but it doesn&amp;rsquo;t scale well to larger clusters.
Available config:
Database: boltdb BoltDB: # Path to database file Path: ./funnel-work-dir/funnel.db  </description>
    </item>
    
    <item>
      <title>Events</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/events/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/events/</guid>
      <description> Events </description>
    </item>
    
    <item>
      <title>Funnel Developers</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/development/developers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/development/developers/</guid>
      <description>Developer Tools Funnel uses:
 Go for the majority of the code. Task Execution Schemas for task APIs. Protobuf + gRPC for RPC communication. gRPC Gateway for HTTP communication. Angular and SASS for the web dashboard. GNU Make for development tasks. Docker for executing task containers. vendetta for Go dependency vendoring. and more.  Prerequisites These are the tools you&amp;rsquo;ll need to install:
 Go 1.8 Make Docker (tested with v1.</description>
    </item>
    
    <item>
      <title>Google Storage</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/google-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/google-storage/</guid>
      <description>Google Storage Funnel supports using Google Storage (GS) for file storage.
The Google storage client is enabled by default, and will try to automatically load credentials from the environment. Alternatively, you may explicitly set the credentials in the worker config:
GoogleStorage: Disabled: false # Path to account credentials file. AccountFile: &amp;quot;&amp;quot;  Example task { &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;gs://funnel-bucket/hello.txt&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/inputs/hello.txt&amp;quot; }], &amp;quot;outputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;gs://funnel-bucket/output.txt&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/outputs/hello-out.</description>
    </item>
    
    <item>
      <title>Grid Engine</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/grid-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/grid-engine/</guid>
      <description>Grid Engine Funnel can be configured to submit workers to Grid Engine by making calls to qsub.
The Funnel server process needs to run on the same machine as the Grid Engine master. Configure Funnel to use Grid Engine by including the following config:
Compute: gridengine GridEngine: |#!/bin/bash #$ -N {{.TaskId}} #$ -o {{.WorkDir}}/funnel-stdout #$ -e {{.WorkDir}}/funnel-stderr {{if ne .Cpus 0 -}} {{printf &amp;#34;#$ -pe mpi %d&amp;#34; .Cpus}} {{- end}} {{if ne .</description>
    </item>
    
    <item>
      <title>HTCondor</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/htcondor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/htcondor/</guid>
      <description>HTCondor Funnel can be configured to submit workers to HTCondor by making calls to condor_submit.
The Funnel server process needs to run on the same machine as the HTCondor master. Configure Funnel to use HTCondor by including the following config:
Compute: htcondor HTCondor: |universe = vanilla getenv = True executable = {{.Executable}} arguments = worker run --config {{.Config}} --task-id {{.TaskId}} log = {{.WorkDir}}/condor-event-log error = {{.WorkDir}}/funnel-stderr output = {{.WorkDir}}/funnel-stdout should_transfer_files = YES when_to_transfer_output = ON_EXIT_OR_EVICT {{if ne .</description>
    </item>
    
    <item>
      <title>HTTP(S)</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/http/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/http/</guid>
      <description>HTTP(S) Funnel supports downloading files from public URLs via GET reqests. No authentication mechanism is allowed. This backend can be used to fetch objects from cloud storage providers exposed using presigned URLs.
The HTTP storage client is enabled by default, but may be explicitly disabled in the worker config:
HTTPStorage: Disabled: false # Timeout for http(s) GET requests. # In nanoseconds. Timeout: 60000000000 # 60 seconds  Example task { &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;http://fakedomain.</description>
    </item>
    
    <item>
      <title>Kafka</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/events/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/events/kafka/</guid>
      <description> Kafka Funnel supports writing task events to a Kafka topic. To use this, add an event writer to the config:
EventWriters: - kafka - log Kafka: Servers: - localhost:9092 Topic: funnel-events  </description>
    </item>
    
    <item>
      <title>Local</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/local/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/local/</guid>
      <description>Local Funnel supports using the local filesystem for file storage.
Funnel limits which directories may be accessed, by default only allowing directories under the current working directory of the Funnel worker.
Config:
LocalStorage: # Whitelist of local directory paths which Funnel is allowed to access. AllowedDirs: - ./ - /path/to/allowed/dir - ...etc  Example task Files must be absolute paths in file:///path/to/file.txt URL form.
{ &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;file:///path/to/funnel-data/hello.</description>
    </item>
    
    <item>
      <title>MongoDB</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/mongodb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/mongodb/</guid>
      <description> MongoDB Funnel supports storing tasks and scheduler data in MongoDB.
Config:
Database: mongodb MongoDB: # Addresses for the seed servers. Addrs: - &amp;quot;localhost&amp;quot; # Database name used within MongoDB to store funnel data. Database: &amp;quot;funnel&amp;quot; Username: &amp;quot;&amp;quot; Password: &amp;quot;&amp;quot;  </description>
    </item>
    
    <item>
      <title>OpenStack Swift</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/swift/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/swift/</guid>
      <description>OpenStack Swift Funnel supports using OpenStack Swift for file storage.
The Swift storage client is enabled by default, and will try to automatically load credentials from the environment. Alternatively, you may explicitly set the credentials in the worker config:
Swift: Disabled: false UserName: &amp;quot;&amp;quot; Password: &amp;quot;&amp;quot; AuthURL: &amp;quot;&amp;quot; TenantName: &amp;quot;&amp;quot; TenantID: &amp;quot;&amp;quot; RegionName: &amp;quot;&amp;quot;  Example task { &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;swift://funnel-bucket/hello.txt&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/inputs/hello.txt&amp;quot; }], &amp;quot;outputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;swift://funnel-bucket/output.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/</guid>
      <description>Overview Funnel makes distributed, batch processing easier by providing a simple task API and a set of components which can easily adapted to a vareity of platforms.
Task A task defines a unit of work: metadata, input files to download, a sequence of Docker containers + commands to run, output files to upload, state, and logs. The API allows you to create, get, list, and cancel tasks.
Tasks are accessed via the funnel task command.</description>
    </item>
    
    <item>
      <title>PBS/Torque</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/pbs-torque/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/pbs-torque/</guid>
      <description>PBS/Torque Funnel can be configured to submit workers to PBS/Torque by making calls to qsub.
The Funnel server process needs to run on the same machine as the PBS master. Configure Funnel to use PBS by including the following config:
Compute: pbs PBS: |#!/bin/bash #PBS -N {{.TaskId}} #PBS -o {{.WorkDir}}/funnel-stdout #PBS -e {{.WorkDir}}/funnel-stderr {{if ne .Cpus 0 -}} {{printf &amp;#34;#PBS -l nodes=1:ppn=%d&amp;#34; .Cpus}} {{- end}} {{if ne .RamGb 0.0 -}} {{printf &amp;#34;#PBS -l mem=%.</description>
    </item>
    
    <item>
      <title>S3</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/s3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/s3/</guid>
      <description>S3 Amazon S3 Funnel supports using AWS S3 for file storage.
The S3 storage client is enabled by default, and will try to automatically load credentials from the environment. Alternatively, you may explicitly set the credentials in the worker config:
AmazonS3: Disabled: false # The maximum number of times that a request will be retried for failures. MaxRetries: 10 Key: &amp;quot;&amp;quot; Secret: &amp;quot;&amp;quot;  Other S3 API Providers Funnel also supports using non-Amazon S3 API providers (Ceph, Cleversafe, Minio, etc.</description>
    </item>
    
    <item>
      <title>Security</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/security/</guid>
      <description> Security </description>
    </item>
    
    <item>
      <title>Slurm</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/slurm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/slurm/</guid>
      <description>Slurm Funnel can be configured to submit workers to Slurm by making calls to sbatch.
The Funnel server process needs to run on the same machine as the Slurm master.
Configure Funnel to use Slurm by including the following config:
Compute: slurm Slurm: |#!/bin/bash #SBATCH --job-name {{.TaskId}} #SBATCH --ntasks 1 #SBATCH --error {{.WorkDir}}/funnel-stderr #SBATCH --output {{.WorkDir}}/funnel-stdout {{if ne .Cpus 0 -}} {{printf &amp;#34;#SBATCH --cpus-per-task %d&amp;#34; .Cpus}} {{- end}} {{if ne .</description>
    </item>
    
    <item>
      <title>Storage</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/</guid>
      <description> Storage </description>
    </item>
    
    <item>
      <title>Tasks</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/tasks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/tasks/</guid>
      <description>Tasks A task defines a unit of work:
 metadata input files to download a sequence of Docker containers + commands to run, output files to upload state logs  The example task below downloads a file named hello.txt from S3 and calls cat hello.txt using the alpine container.
{ &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ # URL to download file from. &amp;quot;url&amp;quot;: &amp;quot;s3://funnel-bucket/hello.txt&amp;quot;, # Path to download file to. &amp;quot;path&amp;quot;: &amp;quot;/inputs/hello.</description>
    </item>
    
  </channel>
</rss>