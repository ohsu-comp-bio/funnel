<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Funnel</title>
    <link>https://ohsu-comp-bio.github.io/funnel/</link>
    <description>Recent content on Funnel</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://ohsu-comp-bio.github.io/funnel/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AWS Batch</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/aws-batch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/aws-batch/</guid>
      <description>Amazon Batch This guide covers deploying a Funnel server that leverages DynamoDB for storage and Batch for task execution. You&amp;rsquo;ll need to set up several resources using either the Funnel CLI or through the provided Amazon web console.
Create Required AWS Batch Resources For Funnel to execute tasks on Batch, you must define a Compute Environment, Job Queue and Job Definition. Additionally, you must define an IAM role for your Batch Job Definition.</description>
    </item>
    
    <item>
      <title>AWS S3</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/aws-s3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/aws-s3/</guid>
      <description>AWS S3 Funnel supports using AWS S3 for file storage.
The S3 storage client is enabled by default, and will try to automatically load credentials from the environment. Alternatively, you may explicitly set the credentials in the worker config:
Worker: Storage: S3: Credentials: # AWS Access key ID Key: &amp;quot;&amp;quot; # AWS Secret Access Key Secret: &amp;quot;&amp;quot;  Example task { &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;s3://funnel-bucket/hello.txt&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/inputs/hello.</description>
    </item>
    
    <item>
      <title>Basic Auth</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/security/basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/security/basic/</guid>
      <description>Basic Auth By default, a Funnel server allows open access to its API endpoints, but it can be configured to require basic password authentication. To enable this, include a password in your config file:
Server: Password: abc123  Make sure to properly protect the configuration file so that it&amp;rsquo;s not readable by everyone:
$ chmod 600 funnel.config.yml  To use the password, set the FUNNEL_SERVER_PASSWORD environment variable:
$ export FUNNEL_SERVER_PASSWORD=abc123 $ funnel task list  Known issues The basic auth user is hard-coded to funnel.</description>
    </item>
    
    <item>
      <title>Compute</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/</guid>
      <description> Compute </description>
    </item>
    
    <item>
      <title>Databases</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/</guid>
      <description> Databases </description>
    </item>
    
    <item>
      <title>Deploying a cluster</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/deployment/</guid>
      <description>Deploying a cluster This guide describes the basics of starting a cluster of Funnel nodes. This guide is a work in progress.
A node is a service which runs on each machine in a cluster. The node connects to the Funnel server and reports available resources. The Funnel scheduler process assigns tasks to nodes. When a task is assigned, a node will start a worker process. There is one worker process per task.</description>
    </item>
    
    <item>
      <title>Development</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/development/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/development/</guid>
      <description> Development </description>
    </item>
    
    <item>
      <title>Download</title>
      <link>https://ohsu-comp-bio.github.io/funnel/download/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/download/</guid>
      <description>Download  linux [funnel-linux-amd64-0.4.0.tar.gz] mac [funnel-darwin-amd64-0.4.0.tar.gz] Windows is not supported (yet), sorry!  Funnel is a single binary.
Funnel requires Docker.
Funnel is beta quality. APIs might break, bugs exist, data might be lost.
Install the lastest development version optional In order to build the latest code, run:
$ go get github.com/ohsu-comp-bio/funnel  Funnel requires Go 1.8+. Check out the development docs for more detail.
Release History 0.4.0 Date: Nov 15, 2017</description>
    </item>
    
    <item>
      <title>DynamoDB</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/dynamodb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/dynamodb/</guid>
      <description>DynamoDB Funnel supports storing task data in DynamoDB. Storing scheduler data is not supported currently, so using the node scheduler with DynamoDB won&amp;rsquo;t work. Using AWS Batch for compute scheduling may be a better option.
Available Config:
Server: Database: dynamodb Databases: DynamoDB: # Basename to use for dynamodb tables TableBasename: &amp;quot;funnel&amp;quot; # AWS region Region: &amp;quot;&amp;quot; Credentials: # AWS Access key ID Key: &amp;quot;&amp;quot; # AWS Secret Access Key Secret: &amp;quot;&amp;quot;  Worker config Using DynamoDB with AWS Batch requires that the worker be configured to connect to the database:</description>
    </item>
    
    <item>
      <title>Elasticsearch</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/elasticsearch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/elasticsearch/</guid>
      <description>Elasticsearch Funnel supports storing tasks and scheduler data in Elasticsearch.
Config:
Server: Database: elastic Databases: Elastic: # Prefix to use for indexes IndexPrefix: &amp;quot;funnel&amp;quot; URL: http://localhost:9200  Writing events from the worker The worker can be configured to write events directly to Elasticsearch, which avoids unnecessary RPC traffic to the Funnel server.
Worker: ActiveEventWriters: - log - elastic EventWriters: Elastic: # Prefix to use for indexes IndexPrefix: &amp;quot;funnel&amp;quot; URL: http://localhost:9200  Known issues We have an unpleasant duplication of config between the Worker and Server blocks.</description>
    </item>
    
    <item>
      <title>Embedded</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/boltdb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/boltdb/</guid>
      <description> Embedded By default, Funnel uses an embedded database named BoltDB to store task and scheduler data. This is great for development and a simple server without external dependencies, but it doesn&amp;rsquo;t scale well to larger clusters.
Available config:
Server: Database: boltdb Databases: BoltDB: # Path to database file Path: ./funnel-work-dir/funnel.db  </description>
    </item>
    
    <item>
      <title>Events</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/events/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/events/</guid>
      <description> Events </description>
    </item>
    
    <item>
      <title>Funnel Developers</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/development/developers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/development/developers/</guid>
      <description>Developer Tools Funnel uses:
 Go for the majority of the code. Task Execution Schemas for task APIs. Protobuf + gRPC for RPC communication. gRPC Gateway for HTTP communication. Angular and SASS for the web dashboard. GNU Make for development tasks. Docker for executing task containers. vendetta for Go dependency vendoring. and more.  Prerequisites These are the tools you&amp;rsquo;ll need to install:
 Go 1.8 Make Docker (tested with v1.</description>
    </item>
    
    <item>
      <title>Google Storage</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/google-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/google-storage/</guid>
      <description>Google Storage Funnel supports using Google Storage (GS) for file storage.
The GS client is NOT enabled by default, you must enabled it in the config:
Worker: Storage: GS: # Automatically discover credentials from the environment. - FromEnv: true # Path to account credentials file. AccountFile:  In the near future, Google Storage will be enabled by default. See issue #332.
Example task { &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;gs://funnel-bucket/hello.</description>
    </item>
    
    <item>
      <title>Grid Engine</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/grid-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/grid-engine/</guid>
      <description>Grid Engine Funnel can be configured to submit workers to Grid Engine by making calls to qsub.
The Funnel server process needs to run on the same machine as the Grid Engine master. Configure Funnel to use Grid Engine by including the following config:
Backend: gridengine Backends: GridEngine: |#!/bin/bash #$ -N {{.TaskId}} #$ -o {{.WorkDir}}/funnel-stdout #$ -e {{.WorkDir}}/funnel-stderr {{if ne .Cpus 0 -}} {{printf &amp;#34;#$ -pe mpi %d&amp;#34; .Cpus}} {{- end}} {{if ne .</description>
    </item>
    
    <item>
      <title>HTCondor</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/htcondor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/htcondor/</guid>
      <description>HTCondor Funnel can be configured to submit workers to HTCondor by making calls to condor_submit.
The Funnel server process needs to run on the same machine as the HTCondor master. Configure Funnel to use HTCondor by including the following config:
Backend: htcondor Backends: HTCondor: |universe = vanilla getenv = True executable = {{.Executable}} arguments = worker run --config {{.Config}} --task-id {{.TaskId}} log = {{.WorkDir}}/condor-event-log error = {{.WorkDir}}/funnel-stderr output = {{.</description>
    </item>
    
    <item>
      <title>Kafka</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/events/kafka/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/events/kafka/</guid>
      <description> Kafka Funnel supports writing task events to a Kafka topic. To use this, add an event writer to the worker config:
Worker: ActiveEventWriters: - kafka - log - rpc EventWriters: Kafka: Servers: - localhost:9092 Topic: funnel-events  </description>
    </item>
    
    <item>
      <title>Local</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/local/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/local/</guid>
      <description>Local Funnel supports using the local filesystem for file storage.
Funnel limits which directories may be accessed, by default only allowing directories under the current working directory of the Funnel worker.
Config:
Worker: Storage: Local: # Whitelist of local directory paths which Funnel is allowed to access. AllowedDirs: - ./ - /path/to/allowed/dir - ...etc  Example task Files must be absolute paths in file:///path/to/file.txt URL form.
{ &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ &amp;quot;url&amp;quot;: &amp;quot;file:///path/to/funnel-data/hello.</description>
    </item>
    
    <item>
      <title>MongoDB</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/databases/mongodb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/databases/mongodb/</guid>
      <description>MongoDB Funnel supports storing tasks and scheduler data in MongoDB.
Config:
Server: Database: mongodb Databases: MongoDB: # Addresses for the seed servers. Addrs: - &amp;quot;localhost&amp;quot; # Database name used within MongoDB to store funnel data. Database: &amp;quot;funnel&amp;quot; Username: &amp;quot;&amp;quot; Password: &amp;quot;&amp;quot;  Writing events from the worker The worker can be configured to write events directly to Mongo, which avoids unnecessary RPC traffic to the Funnel server.
Worker: ActiveEventWriters: - log - mongodb EventWriters: MongoDB: Addrs: - &amp;quot;localhost&amp;quot; Database: &amp;quot;funnel&amp;quot; Username: &amp;quot;&amp;quot; Password: &amp;quot;&amp;quot;  Known issues We have an unpleasant duplication of config between the Worker and Server blocks.</description>
    </item>
    
    <item>
      <title>OpenStack Swift</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/swift/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/swift/</guid>
      <description>OpenStack Swift Funnel supports using OpenStack Swift for file storage.
The Swift client is NOT enabled by default, you must explicitly give the credentials in the worker config:
Worker: Storage: Swift: UserName: Password: AuthURL: TenantName: TenantID: RegionName:  The config currently only supports OpenStack v2 auth. See issue #336.
As always, if you set the password in this file, make sure you protect it appropriately. Alternatively, the Swift client can pull credentials from these environment variables: https://godoc.</description>
    </item>
    
    <item>
      <title>Overview</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/</guid>
      <description>Overview Funnel makes distributed, batch processing easier by providing a simple task API and a set of components which can easily adapted to a vareity of platforms.
Task A task defines a unit of work: metadata, input files to download, a sequence of Docker containers + commands to run, output files to upload, state, and logs. The API allows you to create, get, list, and cancel tasks.
Tasks are accessed via the funnel task command.</description>
    </item>
    
    <item>
      <title>PBS/Torque</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/pbs-torque/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/pbs-torque/</guid>
      <description>PBS/Torque Funnel can be configured to submit workers to PBS/Torque by making calls to qsub.
The Funnel server process needs to run on the same machine as the PBS master. Configure Funnel to use PBS by including the following config:
Backend: pbs Backends: PBS: |#!/bin/bash #PBS -N {{.TaskId}} #PBS -o {{.WorkDir}}/funnel-stdout #PBS -e {{.WorkDir}}/funnel-stderr {{if ne .Cpus 0 -}} {{printf &amp;#34;#PBS -l nodes=1:ppn=%d&amp;#34; .Cpus}} {{- end}} {{if ne .RamGb 0.</description>
    </item>
    
    <item>
      <title>Security</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/security/</guid>
      <description> Security </description>
    </item>
    
    <item>
      <title>Slurm</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/compute/slurm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/compute/slurm/</guid>
      <description>Slurm Funnel can be configured to submit workers to Slurm by making calls to sbatch.
The Funnel server process needs to run on the same machine as the Slurm master.
Configure Funnel to use Slurm by including the following config:
Backend: slurm Backends: SLURM: |#!/bin/bash #SBATCH --job-name {{.TaskId}} #SBATCH --ntasks 1 #SBATCH --error {{.WorkDir}}/funnel-stderr #SBATCH --output {{.WorkDir}}/funnel-stdout {{if ne .Cpus 0 -}} {{printf &amp;#34;#SBATCH --cpus-per-task %d&amp;#34; .Cpus}} {{- end}} {{if ne .</description>
    </item>
    
    <item>
      <title>Storage</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/storage/</guid>
      <description> Storage </description>
    </item>
    
    <item>
      <title>Tasks</title>
      <link>https://ohsu-comp-bio.github.io/funnel/docs/tasks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ohsu-comp-bio.github.io/funnel/docs/tasks/</guid>
      <description>Tasks A task defines a unit of work:
 metadata input files to download a sequence of Docker containers + commands to run, output files to upload state logs  The example task below downloads a file named hello.txt from S3 and calls cat hello.txt using the alpine container.
{ &amp;quot;name&amp;quot;: &amp;quot;Hello world&amp;quot;, &amp;quot;inputs&amp;quot;: [{ # URL to download file from. &amp;quot;url&amp;quot;: &amp;quot;s3://funnel-bucket/hello.txt&amp;quot;, # Path to download file to. &amp;quot;path&amp;quot;: &amp;quot;/inputs/hello.</description>
    </item>
    
  </channel>
</rss>